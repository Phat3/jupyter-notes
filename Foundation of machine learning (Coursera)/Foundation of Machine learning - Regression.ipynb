{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-align:center; color:red'>Foundation of Machine learning</h1>\n",
    "\n",
    "\n",
    "<p style='text-align:center'>**ALL THE IMAGES ARE CREATED AND BELONG TO THE COURSE <a href='https://www.coursera.org/learn/ml-foundations'> Machine Learning Foundations: A Case Study Approach</a> MADE BY University of Washington**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Foundation-of-Machine-learning\">Foundation of Machine learning</a></div><div class=\"lev1\"><a href=\"#Regeression\">Regeression</a></div><div class=\"lev2\"><a href=\"#Use-case---Predicting-the-value-of-a-house\">Use case - Predicting the value of a house</a></div><div class=\"lev2\"><a href=\"#Naive-approach\">Naive approach</a></div><div class=\"lev2\"><a href=\"#Linear-regression\">Linear regression</a></div><div class=\"lev2\"><a href=\"#Residual-sum-of-squares-(RSS)\">Residual sum of squares (RSS)</a></div><div class=\"lev3\"><a href=\"#Retrieving-the-best-price\">Retrieving the best price</a></div><div class=\"lev3\"><a href=\"#Problems\">Problems</a></div><div class=\"lev2\"><a href=\"#High-order-regression\">High order regression</a></div><div class=\"lev2\"><a href=\"#How-can-we-choose-the-proper-model?\">How can we choose the proper model?</a></div><div class=\"lev3\"><a href=\"#Training-/-Test-curves\">Training / Test curves</a></div><div class=\"lev3\"><a href=\"#Adding-other-features\">Adding other features</a></div><div class=\"lev2\"><a href=\"#Regression-pipeline-summary\">Regression pipeline summary</a></div><div class=\"lev1\"><a href=\"#Classification\">Classification</a></div><div class=\"lev2\"><a href=\"#Use-case---Intelligent-restaurant-classifier\">Use case - Intelligent restaurant classifier</a></div><div class=\"lev2\"><a href=\"#Linear-classifier\">Linear classifier</a></div><div class=\"lev3\"><a href=\"#Problems\">Problems</a></div><div class=\"lev2\"><a href=\"#Decision-boundaries\">Decision boundaries</a></div><div class=\"lev2\"><a href=\"#Training-a-classifier-(learning-the-weights)\">Training a classifier (learning the weights)</a></div><div class=\"lev3\"><a href=\"#Test-the-evaluation-of-the-weights\">Test the evaluation of the weights</a></div><div class=\"lev3\"><a href=\"#What-is-a-good-accuracy?\">What is a good accuracy?</a></div><div class=\"lev3\"><a href=\"#Types-of-errors\">Types of errors</a></div><div class=\"lev2\"><a href=\"#Learning-curves\">Learning curves</a></div><div class=\"lev2\"><a href=\"#Level-of-confidence\">Level of confidence</a></div><div class=\"lev2\"><a href=\"#Classification-pipeline-summary\">Classification pipeline summary</a></div><div class=\"lev1\"><a href=\"#Clustering-and-similarity\">Clustering and similarity</a></div><div class=\"lev2\"><a href=\"#Use-case---Document-retrieval-task\">Use case - Document retrieval task</a></div><div class=\"lev2\"><a href=\"#Characterize-the-document\">Characterize the document</a></div><div class=\"lev3\"><a href=\"#Bag-of-words-model\">Bag of words model</a></div><div class=\"lev3\"><a href=\"#Problems\">Problems</a></div><div class=\"lev4\"><a href=\"#Vector-normalization\">Vector normalization</a></div><div class=\"lev3\"><a href=\"#Prioritize-words\">Prioritize words</a></div><div class=\"lev4\"><a href=\"#Rare-words\">Rare words</a></div><div class=\"lev4\"><a href=\"#Important-words\">Important words</a></div><div class=\"lev3\"><a href=\"#Term-frequency---Inverse-document-frequency-(TF-IDF)\">Term frequency - Inverse document frequency (TF-IDF)</a></div><div class=\"lev2\"><a href=\"#Retrieve-related-documents\">Retrieve related documents</a></div><div class=\"lev2\"><a href=\"#Discover-cluster-of-related-documents\">Discover cluster of related documents</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regeression\n",
    "\n",
    "**REGRESSION** : We have a set of features and we wanna model how this feature change when the values of the feature changes.\n",
    "\n",
    "We can also use regression for **classification** (heavily used in the SPAM filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case - Predicting the value of a house\n",
    "\n",
    "In regression we refer to the axis as:\n",
    "- **x variable** --> **feature, covariate or predictor**\n",
    "- **y variable** --> **observation or response**\n",
    "\n",
    "## Naive approach\n",
    "1. Consider the aspects of your house\n",
    "2. Look at other recent sales that have occurred in my neighborhood.\n",
    "3. Plot a graph of the sales price(y-axis) and the square feet(x-axys) of the houses sold recently in my neighborhood.\n",
    "4. Look at how big my house is and look for other sales of houses of roughly the same size of mine (In a certain range).\n",
    "5. I can't end up having a few valid data and discard all the other information coming from other data not inside my range.\n",
    "\n",
    "**NOT CORRECT!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "We can build a model the relationship between the square footage of the house and the house sales price using **linear regression**.\n",
    "\n",
    "The model is notingh more than a line having the following equation:\n",
    "\n",
    "<p style='text-align:center'>**fW(x) = W0 + W1*x**</p>\n",
    "<p style='text-align:center'>**W = (W0, W1)**</p>\n",
    "\n",
    "<img src=\"CM71THH7NOKQPHKUU4GR6D7IPBE2L3MG.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **W0** = intercept \n",
    "- **W1** = slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find out the **best possible values** for **W0** and **W1**!!\n",
    "\n",
    "We have to define a cost for a given line using **residual sum of squares (RSS)**\n",
    "\n",
    "## Residual sum of squares (RSS)\n",
    "We take the candidate line and for each observation we look how far is that observation from our line. This will give us the **distance between the real value (the observation) and the value we predict (the point on the line)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"B0DXKTKXN6MAU5L162UY5T99RJ0LGWPT.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance is squared and RSS is calculated following this formula:\n",
    "<p style='text-align:center'>**RSS(W0, W1) = ( price(house1) - (W0 + W1*square_feet(house1) )^2 + ... + ( price(houseN) - (W0 + W1*square_feet(houseN) )^2**</p>\n",
    "\n",
    "The best candidate **Wcan = (W0can, W1can)** is the one which **MINIMIZE** the RSS value. This value will be the best candidate because the line produced with this parameters has, overall, the observations closest to the predictions made, so the predictions tend to be good.\n",
    "\n",
    "### Retrieving the best price\n",
    "it is sufficient to substitute the predictor (my house square feet) in the retrived model in order to find the best possible response (sale price).\n",
    "<p style='text-align:center'>**best_price = W0can + W1can*square_feet(myHouse)**</p>\n",
    "\n",
    "### Problems\n",
    "\n",
    "Are we sure that the best model for our problem is linear? It could also be **quadratic** or more.\n",
    "\n",
    "## High order regression\n",
    "It is a sub-type of linear regression (actually everybody refers to it as linear regression).\n",
    "\n",
    "If we take into account the quadratic regression the model is no more an equation that defines a straight line but it is a **parabola**:\n",
    "<p style='text-align:center'>**fW(x) = W0 + W1*x + W2*x^2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"VUOF44KIMPF68XS35D6DD8F5JD40WTTQ.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same formula of the RSS calculation, substituting the parabola equation in the distance calculus, we can verify if this model is better or worse than the linear one.\n",
    "\n",
    "We can increase the order of the function used to build the model but it can produce wrong results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"HXTXUDL8V42ELFJYSRKCDLXVJUBDO8OI.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the RSS value is the minimum possible one **BUT** the model is wrong: the value of my house predicted by this model is **TOO LOW**!!\n",
    "\n",
    "This problem is called <span style='color:red'>OVERFITTING</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we choose the proper model?\n",
    "The key idea is that we want to simulate predictions on observations that we already have.\n",
    "\n",
    "First of all, we have to split our dataset into two categories;\n",
    "- **Training set** : Data used to fit our model (blue dots).\n",
    "- **Test set** : Data use in order to validate our predictions (gray dots).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"I6H3C7RFDXDTCB0R87TMLI2AGYI68J2G.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to calculate the **training error** thai is the RSS **only** of the data in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"Y6QHFXXLWJOC5WX9VEQOBT7DDLL74P20.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same calculus in order to retrieve the **test error** (The RSS calculated only on the data included in the test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"XWMIWSBX54TSNG1PXU4V0K9BE14QEJYJ.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Test curves\n",
    "As we saw before, the higher the polynomial that builds our model is the lower will be our training error (the RSS calculated only on the training set). This happens because the line can touch perfectly every single data of our model, but this is not an indication of correctness (Remember the **OVERFITTING** problem!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"FNJUXODVTK27HJL2FMST6O19LS9QY4XK.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the test error things are different: the error decrease until a certain model complexity but after that point the error will increase, detecting the overfitting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"PRHGV4QP8RNX6SW8UL1O4CIENH0ISPY3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding other features\n",
    "In this example, the square feet feature of the house is not sufficient in order to make a good prediction. We can have an house that has the same square feet as ours but only one bathroom instead of three.\n",
    "\n",
    "We can add other feature increasing the dimensions of our graph (for example adding the number of bathrooms on the z-axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"QPQODESA9FKX5GV0LJ3V8E5696PS9SRS.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model won't be a line anymore but a **plane** identified by this equation:\n",
    "<p style='text-align:center'>**fW(x) = W0 + W1*square_feet + W2*#bathrooms**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"I6NL8B94HEM5L5Q4LJKMRS740Q82JIVO.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression pipeline summary\n",
    "We can summarize the whole process of regression described so far with this picture: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"N0I182O9KRI64CLGFC7JM704SBSGXLLN.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start from a training data with a lot of features.\n",
    "2. With the feature extraction component select only the important features.\n",
    "3. With a machine learning model, regression in this case, predict the values we want.\n",
    "4. Validate the predictions with the test set in the quality metric component.\n",
    "5. With a machine learning algorithm adjust the parameters of the model with respect to the results of the quality metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "**Classification** : classification is the problem of identifying to which of a set of categories a new observation belongs, on the basis of a training set of data containing observations whose category membership is known.\n",
    "\n",
    "## Use case - Intelligent restaurant classifier\n",
    "\n",
    "1. Get all the reviews about one restaurant\n",
    "2. Break all reviews into sentence\n",
    "3. Select only the sentences regarding the topic in which we are interested (sushi for example)\n",
    "4. Pass these sentences as input into a **sentence sentiment classifier** in order to extract if a sentence is positive or negative.\n",
    "5. Create the prediction about the quality of the topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifier\n",
    "\n",
    "A simple linear classifier takes as input:\n",
    "* The sentence that has to be classified\n",
    "* A list of words with their weights (positive words weight > 0; negative words weight < 0)\n",
    "\n",
    "The classifier then sums up the weight of each word inside the sentence and it classifies the sentence as follow:\n",
    "* **result > 0 --> positive sentence**\n",
    "* **result < 0 --> negative sentence**\n",
    "\n",
    "This type of classifier is called **linear** because **outputs are weighted sum of inputs**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"XFYYSPNO0W9QWH5MY6CBPT3V8I1407L4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "* How do we get the list of positive and negative words?\n",
    "\n",
    "\n",
    "* Words have different degree of sentiment (**awesome** is more positive than **good**)\n",
    "\n",
    "These two problems are addressed by **learning classifier**\n",
    "\n",
    "* Single words are not enough (**good** is positive but **not good** is negative)\n",
    "\n",
    "This problem instead is addressed by **elaborate features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundaries\n",
    "We need to find a threshold that differentiates the positive sentence from the negative ones.\n",
    "\n",
    "This threshold is called **decision boundary**\n",
    "\n",
    "If we consider only two words, awful and awesome for example, weighed respectively -1.5 and 1.0 the decision boundary is retrieved in this way:\n",
    "\n",
    "<p style='text-align:center'>**1.0#awesome -1.5#awful = 0**</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"HLCK21LOA6YWXJG902ECUOE4PK3W2LBR.png\"/>\n",
    "\n",
    "In this case the decision boundary is represented by a line (we have only two words so only two dimensions).\n",
    "If we have had:\n",
    "\n",
    "* 3 words --> plane\n",
    "\n",
    "* more than 3 words --> hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier (learning the weights)\n",
    "In order to train a classifier we have to:\n",
    "1. Get a dataset of sentences already marked as positive or negative\n",
    "2. Split the dataset into a training set and a test set\n",
    "3. feed the training set as input to the **learn classifier** (this learn classifier is gonna learn the appropriate weights for the words)\n",
    "4. The weights learned by the learn classifier are used to score every element in the test set and evaluate how good is our learn classifier \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"UR5IRR9MPVS4MOHS37UJDYQYG21EKS55.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the evaluation of the weights\n",
    "In order to test the goodness of the retrieved weights we need to:\n",
    "1. Feed a number of sentences, already known to be positive or negative, to our classifier \n",
    "2. Get the prediction made by our classifier\n",
    "3. Compare the prediction with the real sentiment of the sentence\n",
    "4. If the prediction matches the real sentiment than correct++ otherwise mistakes++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"BKP2NX0BEFL537QSDB853NE55TXB48OC.png\"/>\n",
    "\n",
    "The **error rate of the classifier** is then calculated as:\n",
    "<p style='text-align:center'>**classifier_error = #mistakes / #sentences**</p>\n",
    "\n",
    "while the **accuracy of the classifier** is retrieved as:\n",
    "<p style='text-align:center'>**classifier_accuracy = #correct / #sentences**</p>\n",
    "\n",
    "obviously the error and the accuracy are bound together:\n",
    "<p style='text-align:center'>**classifier_error = 1 - classifier_accuracy**</p>\n",
    "\n",
    "### What is a good accuracy?\n",
    "At **least** our accuracy must be better than the random guesses on data (for example if I have only two classes of decision like positive and negative, my accuracy must be > 50%).\n",
    "\n",
    "Given that a high accuracy rate isn't always a good indicator of the quality of our classifier. For example if we consider the SPAM email example the data shows that 90% of the emails sent are SPAM; if we have a classifier with a 90% accuracy predicting every email sent is SPAM gives us a 90% of accuracy but actually **our classifier always make the SAME prediction for every email!!!**.\n",
    "\n",
    "This problem is called <span style='color:red'>MAJORITY CLASS PREDICTION</span>.\n",
    "\n",
    "This problem causes the **impression that our classifier has amazing performance when there is class imbalance (one class is more common than others)** even if it was poorly designed.\n",
    "\n",
    "\n",
    "So how can we design a good classifier? we have to ask ourself the following questions:\n",
    "* Is there class imbalance?\n",
    "\n",
    "\n",
    "* How does it compare to random guessing?\n",
    "\n",
    "\n",
    "* What accuracy does my application need?\n",
    "\n",
    "### Types of errors\n",
    "The types of error are given by the relationship between the **predicted labels** and the **true labels**. Actually we can have:\n",
    "* **False positives --> True label negative - predicted label positive**\n",
    "\n",
    "* **False negatives --> True label positive - predicted label negative**\n",
    "\n",
    "The relationship between the labels are summarized by the **confusion matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"A5BCMUQER2MIY183E4UTYLPODW841E4F.png\"/>\n",
    "\n",
    "This matrix can give us some clues about our prediction model and how to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we can see that a person that has cold can be confused both as a healthy person or one who has a Flu so it could be better to improve our model to better classifies these patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves\n",
    "A learning curve relates the amount of data that we have for training set with the error that we are making.\n",
    "\n",
    "The more data we have the better the model will be **as long as the QUALITY of data is high**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"FPURUS44J8RAWL4TDGQFM7LI3BTWJWKY.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the example increasing the amount of data for the training set will decrease the test error, but this error can not decrease forever: there will be always a gap from the error and 0. This gap is called **BIAS**.\n",
    "\n",
    "In order to decrease the bias we have to increase the complexity of our model, for example for a sentiment analysis model consider bigrams instead of single words can decrease the bias, but the amount of data needed for the training set increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"TB3ITO3SG73P3J2R6ADG2MM065IABJ2J.png\"/>\n",
    "\n",
    "## Level of confidence\n",
    "Usually a classifier does not return only the predicted label but it returns also the **confidence level** related to that prediction.\n",
    "\n",
    "The confidence level is given by the following conditional probability:\n",
    "\n",
    "<p style='text-align:center'>**confidence_level = P(predicted_label | input_sentence)**</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification pipeline summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"HRNN8T2HWP2S0M5UOL5FIJG90NLG62DR.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start from a training data that has reviews and their sentiments\n",
    "2. With the feature extraction component extract the word counts.\n",
    "3. With a machine learning model, classification in this case, predict the sentiment related to the input review .\n",
    "4. Feed the prediction made to the quality metric component and compare it with the real sentiment related to that review.\n",
    "5. With a machine learning algorithm adjust the weight for each word in order to improve classification accuracy.\n",
    "\n",
    "# Clustering and similarity\n",
    "\n",
    "**Clustering and similarity** : we have a lot of observations and we want to infer some kind of structure underlying these observations.\n",
    "\n",
    "Groups of related observation are called **clusters**.\n",
    "\n",
    "## Use case - Document retrieval task\n",
    "\n",
    "1. You are reading an article you like\n",
    "2. Infer its characteristics \n",
    "3. Search for similar articles based on the extracted characteristics\n",
    "4. Propose the retrieved articles\n",
    "\n",
    "## Characterize the document\n",
    "In order to characterize the document we are reading we can use the model known as <span style='color:red'>BAG OF WORDS MODEL</span> \n",
    "\n",
    "### Bag of words model\n",
    "* Ignore the order of words\n",
    "* Get a dictionary of the language in which the document is written\n",
    "* Count the number of instances of each word in the document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"HA7BK767IEPJ2BIX9YYH293PP59U8Y2I.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us a vector that describe our document.\n",
    "\n",
    "Now we have to find which other documents are similar to this one comparing their characteristic vector. The **similarity** is obtained by multiplying the content of cells in the same index between two vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"TBJY35PIQ4N2GHY2U9RN19S8J433DQ4V.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the similarity between the two documents is **13**\n",
    "\n",
    "### Problems\n",
    "In the method explained above we don't take into account the length of the document itself, so the characteristics vector is not so meaningful. For example if we duplicate the same two documents illustrated before, the similarity value would not be 13 anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"W6OFW7KA77CDULICNXQ9SY55IULB0407.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid this problem we have to **normalize (i. e. compute the norm of the vector)** the vectors an **then** calculate the similarity.\n",
    "\n",
    "#### Vector normalization\n",
    "Given a Vector **V** of dimension **n** with **m** elements, the **norm** is calculated as follow:\n",
    "<p style='text-align:center'>**||Vn|| = sqrt^n(x1^n + x2^n + ... + xm^n)**</p>\n",
    "Then divide each element in the vector by the retrieved norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"SKO0LDPOKIP3FJQ7A1CQQBGU536D7RAU.png\"/>\n",
    "\n",
    "### Prioritize words\n",
    "We need to distinguish between **common words** and **rare words**. The common words can be articles like \"the\", \"a\" etc... while rare words are very specific words bound to the topic of the document. Obviously these rare words describe better the documents so we need a way to identify them and prioritize this type of words over the common ones.\n",
    "\n",
    "#### Rare words\n",
    "They appears infrequently inside the dataset and in order to prioritize their score we need to **discount the score of common words**.\n",
    "\n",
    "In order to do that we have to count how many times the word W appear in the dataset. The score of the word W will be discounted as much as the number of occurrences is high.\n",
    "\n",
    "#### Important words\n",
    "Consider only the rare word is **meaningless**. We have to consider the **important words**. \n",
    "\n",
    "An important word is characterized by:\n",
    "1. Appears frequently in the document analyzed --> **common locally**\n",
    "2. Appears rarely in the dataset --> **rare globally**\n",
    "\n",
    "This trade-off between something that is common locally but rare globally is called **Term frequency - Inverse document frequency (TF-IDF)**\n",
    "\n",
    "### Term frequency - Inverse document frequency (TF-IDF)\n",
    "TF-IDF vector will give us the real characteristic of the document.\n",
    "\n",
    "We have to do the following steps:\n",
    "* Calculate the terms frequency in the document (as before)\n",
    "* Calculate the inverse document frequency of the terms considering the whole dataset following this formula:\n",
    "\n",
    "<p style='text-align:center'>**IDF(wordX) = log( #docs / 1 + #docs_containing_wordX )**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"UCK75OQ0JC29H81NMWTXT8Q51H02XKAQ.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate the vector TF-IDF multipliying the content of cells having the same index of vectors term frequency and inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"BC58NY5WVPKHCSH6UVYQBKPJACTQLENQ.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve related documents\n",
    "Having the method to characterize a document now we need a way to retrieve all those documents similar to the interesting one.\n",
    "\n",
    "We can use the algorithm called **neighbor search** :\n",
    "1. For each documents in the dataset compute the similarity between the queried document and the analyzed one\n",
    "2. return the k documents which have the best similarity values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover cluster of related documents\n",
    "If we have a document and we want to assign it to a **predefined label** this is no more a clustering problem **but** a **CLASSIFICATION** one (**supervised learning task**).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead when **no labels** are provided we the problem becomes a **CLUSTERING** problem (**unsupervised learning task**).\n",
    "\n",
    "The process of clustering is defined as the process that given a vector of docs with their characteristics, and no labels provided, returns cluster labels for the inputs documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"JYYPUAFWLPX03PHE2MFRR3FTSL6RFAS3.png\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "toc_cell": true,
   "toc_number_sections": false,
   "toc_section_display": "none",
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "toc_position": {
   "height": "546px",
   "left": "9.125px",
   "right": "20px",
   "top": "121px",
   "width": "292px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
